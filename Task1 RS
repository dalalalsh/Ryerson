#Recommender Systems for Task1 dataset

library(readr)

# Load dataset
# Load train data
trainold = read_csv("C:/Users/dalal/Downloads/DataminingContest2009.Task1.Train.Inputs", 
                    col_types = cols(zip1 = col_integer()))

# Load target data (0 = legit, 1 = fraud)
target = read_csv("C:/Users/dalal/Downloads/DataminingContest2009.Task1.Train.Targets", 
                  col_names = FALSE)
# Re-name the column as "target"
colnames(target)[c(1,1)] = c("target")  #name the column

# Combine train and target data
train = cbind(trainold, target)

# Remove NAs
train = na.omit(train)
train$total = NULL
train$hour2 = NULL

#capping outliers
qnt = quantile(train$flag5, probs=c(.25, .75), na.rm = T)
caps = quantile(train$flag5, probs=c(.05, .95), na.rm = T)
H = 1.5*IQR(train$flag5, na.rm = T)
train$flag5[train$flag5 < (qnt[1] - H)] = caps[1]
train$flag5[train$flag5 > (qnt[2] + H)] = caps[2]

# Recommender Systems 

# install following three packages in R
library(recommenderlab)
library(reshape2)
library(ggplot2)
require(devtools) 

# Get additional recommendation algorithms
install_github("sanealytics", "recommenderlabrats")

# Remove ALL column EXEPT custAttr1, custAttr2, target. We do not need it 
tr = train[,c("domain1", "hour1", "field1", "flag1", "flag2", "flag3", "target")]

#trmelt = melt(tr, id = "domain1", measure = c("target"))

# Using acast to convert above data
g = acast(tr, domain1 ~ field1 + flag1 + flag2 + flag3, value.var = target, fill=NULL)

#try: g = acast(tr, domain1 ~ field1 + flag1 + flag2 + flag3)
#try: data1 <- melt(tr, id.vars = c("domain1", "target"))
#   gg = acast(data1, domain1 ~ variable + target)

# Check the class of g
class(g)


# Convert it as a matrix
R = as.matrix(g)

# Convert R into realRatingMatrix data structure
#   realRatingMatrix is a recommenderlab sparse-matrix like data-structure
r <- as(R, "realRatingMatrix")
r@data@x[r@data@x > 1] = 1
r 



# Exploring similarity data 

# first 6 users
similarity_users <- similarity(r[1:6, ], method = "cosine", which = "users")
as.matrix(similarity_users)
image(as.matrix(similarity_users), main = "User similarity")

# First 6 concatenated features
similarity_items <- similarity(r[, 1:6], method ="cosine", which = "items")
as.matrix(similarity_items)
image(as.matrix(similarity_items), main = "Features similarity")

# Heatmap of the rating matrix
#image(r, main = "Heatmap of the rating matrix") # hard to read
image(r[1:20, 1:40], main = "Heatmap of the first 20 rows and 25 columns")

# Normalizing data
ratings_movies_norm <- normalize(r)
sum(rowMeans(ratings_movies_norm) > 0.00001)

image(ratings_movies_norm[1:20,1:40], main = "Heatmap of the top users and movies")

# Turn it into data-frame
head(as(r, "data.frame"))


# Defining training/test sets
# 1. Cross Validation
n_fold <- 10
scheme <- evaluationScheme(data = r, 
                              method = "cross-validation",
                              k = n_fold, 
                              given = 14)
size_sets <- sapply(scheme@runsTrain, length)
size_sets

#OR
# 2. Spliting data in to 80% training and 20% testing
scheme = evaluationScheme(r, method = "split", train = .8, given = 39, k = 1)  ######### 10 to 40
scheme


### Method 1


# creation of recommender model based on ubcf
Rec.ubcf <- Recommender(getData(scheme, "train"), "UBCF")
# creation of recommender model based on ibcf for comparison
Rec.ibcf <- Recommender(getData(scheme, "train"), "IBCF")

# making predictions on the test data set
p.ubcf <- predict(Rec.ubcf, getData(scheme, "known"), type="ratings")
# making predictions on the test data set
p.ibcf <- predict(Rec.ibcf, getData(scheme, "known"), type="ratings")

p.ubcf@data@x = ifelse(p.ubcf@data@x > 0.5,1,0)
p.ibcf@data@x = ifelse(p.ibcf@data@x > 0.5,1,0)

# obtaining the error metrics for both approaches and comparing them
# RMSE, MSE, and MAE
error.ubcf<-calcPredictionAccuracy(p.ubcf, getData(scheme, "unknown"))
error.ibcf<-calcPredictionAccuracy(p.ibcf, getData(scheme, "unknown"))
error <- rbind(error.ubcf,error.ibcf)
rownames(error) <- c("UBCF","IBCF")

# Accuracy
accuracy = c((1 - error[1,3])*100, (1 - error[2,3])*100)
error = cbind(error, accuracy)
error



### Method 2
# Create a recommender object (model)
#   Run anyone of the following four code lines.
#     Do not run all four
#       They pertain to four different algorithms.
#        UBCF: User-based collaborative filtering
#        IBCF: Item-based collaborative filtering
#      Parameter 'method' decides similarity measure
#        Cosine or Jaccard
rec=Recommender(getData(scheme,"train"),method="UBCF", param=list(method="Cosine"))
rec=Recommender(trainData,method="UBCF", param=list(normalize = "Z-score",method="Jaccard",nn=5, minRating=0))
rec=Recommender(trainData,method="IBCF", param=list(method="Jaccard",minRating=0))
rec=Recommender(trainData,method="POPULAR")
rec=Recommender(trainData,method="SVD")

# Depending upon your selection, examine what you got
print(rec)
getModel(rec)



############Create predictions#############################
recom <- predict(rec, getData(scheme,"train"), type="ratings")
recom



########## Examination of model & experimentation  #############
# Not done yet
